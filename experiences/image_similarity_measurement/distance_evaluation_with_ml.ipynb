{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Purpose of this notebook**\n",
    "\n",
    "This notebook presents how we can use the machine learning the similarity between images.\n",
    "Particularly we would like to distinguish several types of relationship between images:\n",
    "1. exact duplicate\n",
    "1. near-duplicate\n",
    "1. similar\n",
    "1. different\n",
    "\n",
    "![alt text](categories_similarity_openfoodfact.jpg \"Title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Proposal**  \n",
    "Use the machine learning to represent the image in a new space where the distance is correlated with similarity.\n",
    "\n",
    "**Hypothesis**  \n",
    "The deep learning with neural network (NN) is supposed to be able to catch/learn some patterns from its training dataset that helps itself to discriminate instance of this dataset. By using the trained neural networks, it will be possible to represent the picture in some embeddings that `would be easier to discriminate`, or allow us to build `a metric of similarity`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Protocol**  \n",
    "1. load images\n",
    "1. download a already trained NN for images\n",
    "2. use the backbone of the model to generate the embeddings of images (more exactly to transform the pixel of images into another representation called embeddings). Thus, we considered the following hypothesis: `the euclidean distance in the embedding space` is correlated with the `similarity`.\n",
    "1. by products, look at the distance between images, tag some of them that are `exact_duplicate`, `near_duplicate`, `very_similar` and `different`\n",
    "1. build a small model that determine the optimal threshold.\n",
    "\n",
    "\\*: if you do not understand something, be curious :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Protocol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datasets import Dataset, Image, load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to clean data if necessary\n",
    "# import os\n",
    "# path = Path('../data/images').resolve()\n",
    "# for dir in os.listdir(path):\n",
    "#     for file in os.listdir(path / dir):\n",
    "#         if 'front.' in file:\n",
    "#             os.remove(path / f'{dir}/{file}')\n",
    "#         if 'ingredients.' in file:\n",
    "#             os.remove(path / f'{dir}/{file}')\n",
    "#         if 'nutrition.' in file:\n",
    "#             os.remove(path / f'{dir}/{file}')\n",
    "#         if 'packaging' in file:\n",
    "#             os.remove(path / f'{dir}/{file}')\n",
    "#         if 'other' in file:\n",
    "#             os.remove(path / f'{dir}/{file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_dataset(\"imagefolder\", data_dir=\"../../data/images\")\n",
    "images = images['train'].cast_column('image', Image(decode=True)) # all images are in train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load models and produce embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor, AutoModel\n",
    "import torch\n",
    "import torchvision.transforms as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = \"nateraw/vit-base-beans\"\n",
    "extractor = AutoFeatureExtractor.from_pretrained(model_ckpt)\n",
    "model = AutoModel.from_pretrained(model_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candidate_subset = images.filter(lambda x: x['label'] == 1)\n",
    "# candidate_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformation chain.\n",
    "\n",
    "class TransformationChain():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.transformation_chain = T.Compose(\n",
    "        [\n",
    "            # We first resize the input image to 256x256 and then we take center crop.\n",
    "            T.Resize(int((256 / 224) * extractor.size[\"height\"])),\n",
    "            T.CenterCrop(extractor.size[\"height\"]),\n",
    "            T.ToTensor(),\n",
    "        ]\n",
    "    )\n",
    "        \n",
    "    def __call__(self, image):\n",
    "        tensor = self.transformation_chain(image)\n",
    "        if tensor.shape[0] == 1:\n",
    "            tensor = tensor.expand(3, tensor.shape[1], tensor.shape[2])\n",
    "        if tensor.shape[0] > 3:\n",
    "            tensor = tensor[:3]\n",
    "        tensor = T.Normalize(mean=extractor.image_mean, std=extractor.image_std)(tensor)\n",
    "        return tensor\n",
    "\n",
    "transformation_chain = TransformationChain()\n",
    "\n",
    "def extract_embeddings(model: torch.nn.Module):\n",
    "    \"\"\"Utility to compute embeddings.\"\"\"\n",
    "    device = model.device\n",
    "\n",
    "    def pp(batch):\n",
    "        images = batch[\"image\"]\n",
    "        image_batch_transformed = torch.stack(\n",
    "            [transformation_chain(image) for image in images]\n",
    "        )\n",
    "        new_batch = {\"pixel_values\": image_batch_transformed.to(device)}\n",
    "        with torch.no_grad():\n",
    "            embeddings = model(**new_batch).last_hidden_state[:, 0].cpu()\n",
    "        return {\"embeddings\": embeddings}\n",
    "    return pp\n",
    "\n",
    "# to check \n",
    "# for i, image in enumerate(images):\n",
    "#     try:\n",
    "#         transformation_chain(image['image'])\n",
    "#     except:\n",
    "#         print('error:', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here, we map embedding extraction utility on our subset of candidate images.\n",
    "batch_size = 1\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f'{device=}')\n",
    "extract_fn = extract_embeddings(model.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = images.map(extract_fn, batched=True, batch_size=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pacmap import PaCMAP\n",
    "from trimap import TRIMAP\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distance / similarity to determine threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "\n",
    "projector = TRIMAP() # PaCMAP()\n",
    "emb_2d = projector.fit_transform(\n",
    "    np.array(embeddings.filter(lambda x: x['label'] == 1)['embeddings'])\n",
    "    )\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(*emb_2d.T, s=7)\n",
    "plt.show()\n",
    "\n",
    "threshold = 0.2\n",
    "dist = euclidean_distances(emb_2d)\n",
    "\n",
    "mask = (dist > 0).astype(int) * (dist < threshold).astype(int)\n",
    "np.where(mask ==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b = np.where(mask ==1)\n",
    "a = a[len(a)//2]\n",
    "b = b[len(b)//2]\n",
    "if not isinstance(a, list):\n",
    "    a = [a]\n",
    "if not isinstance(b, list):\n",
    "    b = [b]\n",
    "for i, j in zip(a, b):\n",
    "    print(i, j)\n",
    "    embeddings[int(i)]['image'].show()\n",
    "    embeddings[int(j)]['image'].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fddf81331d3d403736c81a6e969efb76dc4d870870bdc04eed7585b0e0dd8d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
